---
layout: post
title: 【论文阅读】weekly paper reading - 6
categories: 论文笔记
keywords: Bio, NLP
mathjax: true
---

[TOC]

# 等变性图神经网络

见分子中的不变性与等变性。

# ICLR21 NEAREST NEIGHBOR MACHINE TRANSLATION
看了kNN-MT的原始论文，我觉得用在文本分子多模态的翻译任务中有这样的问题：
- 如果是文本生成分子，分子只能自回归地生成，适合1D序列表示的分子。印象中也有自回归式生成分子图的，但是我还没怎么了解过。感觉自回归的生成不太适合分子这种数据。
- kNN-MT需要有一个预先训练好的翻译模型，但是文本分子配对的数据很少，构建不出高质量的数据库。
- 分子生成文本这个任务，相比起翻译，更像image caption。对一个东西进行描述，多模问题应该是比翻译更严重的。所以即使有相似的上文表示，下文也不一定相似。

# ICLR22 FILIP: FINE-GRAINED INTERACTIVE LANGUAGEIMAGE PRE-TRAINING

计算两个模态在细粒度上的相似度，来实现更多的模态交互。


![](/images/blog/flip.png)

对于一张被分成n1个patch的图像，和一条被分成n2个token的文本。首先计算token级别上的相似度（n1*n2），再分别对每行（和每个patch最相似的token）/每列（和每个token最相似的patch）取最大，再对这些最大值取平均，得到image-to-text和text-to-image的相似度。然后按照CLIP的方法训练。


# ICLR23 MOLE-BERT- RETHINKING PRE-TRAINING GRAPH NEURAL NETWORKS FOR MOLECULES

## Motivation

- 以往在图上的预训练任务只有AttrMask（模型预测被mask的原子的类型），以前的研究者发现这个任务会有负迁移问题——预训练模型效果还比不上没有预训练的模型。他们认为应该再加上图级别的预训练任务。  
- 本文的作者认为图级别的标签很少，并且设计的任务可能和下游任务没什么关系，反而会降低模型性能。  作者提供了关于负迁移问题的另一种解释：分子的原子词表太小了，并且类别极不平衡（大部分为碳）。  但是即使都是碳原子，根据这个碳所连接的上下文内容不同，它也有不同的性质，比如醛碳和酯碳。这类原子需要区分开，也就达到了扩充元素词表和平衡类别的目的。

## Method
- 作者首先借助VQ-VAE训练一个codebook作为更大的词表，用于后续分词。用GNN对分子图编码，因此每个原子都聚合了上下文的信息，具有了不同的表示。之后利用VQ-VAE将原子的表示映射到codebook中。codebook的大小是512，大于之前用的总元素的词表（118）。为了防止不同的原子映射到同一个code上，先验地设计了C、N、O只能分别映射到[1, 128], [129, 256]、[257, 384]这三种范围的code上。decoder对原来的原子进行重建。

![](/images/blog/molebert.png)

- 词表训好后，设计了两个任务来对GNN预训练：
    - Masked Atoms Modeling (MAM)：和MaskAttr一样，但是此时的词表更大，类别更平衡。
    - triplet masked contrastive learning (TMCL)：之前图级别上的对比学习方法是不管分子的相似度就将不同的分子推远，因此作者根据mask程度不同，分子的相似度也不同，来进行不同程度的对比学习。


# ACL21 Transfer Learning for Sequence Generation from Single-source to Multi-source

## Motivation

之前处理Multi-source sequence generation (MSG) 任务的做法通常是直接将multi-source拼接，用pretrain的生成模型finetune。可能会导致：
- 预训练模型的知识被遗忘
- 预训练模型学习到的attention参数是针对仅输入一种source的，学不好拼接后的多种source之间的交互信息

## Method

- 需要数据集：1. 大量的无标签多种单语数据集（比如有A、B、C三种语言）；2. 双语数据集（A-C、B-C，C为目标语言）；3. 少量的multi-source数据集（AB-C）
- 训练过程：easy-to-hard + 增强不同source的交互
    - 预训练：模型1在A、B、C三种单语数据上预训练
    - 迁移学习（stage 1）：模型2用模型1初始化，在双语数据集上学习如何从A到C和从B到C
    - 迁移学习（stage 2）：模型3用模型2初始化，除此之外加上一个fine encoder模块，整个模型学习如何从AB翻译到C。输入的multi-source仍然是拼接的，但是加入的fine encoder模块会用cross attention增强不同source之间的交互。decoder中的cross- attention也区别对待不同source的表示。

