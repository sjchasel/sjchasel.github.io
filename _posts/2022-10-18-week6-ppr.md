---
layout: post
title: 【论文阅读】weekly paper reading - 4
categories: 论文笔记
keywords: NLP, NMT
mathjax: true
---

# ACL22 Learning Confidence for Transformer-based Neural Machine Translation

## Motivation

估计模型对预测结果的confidence有助于给出准确的失败率预测、对噪声数据和离群数据的风险预测。但是现在的模型softmax后的probability和模型的confidence不一致。这篇论文将模型的confidence定义为：模型需要借助多少提示来做出正确的预测。基于Ask For Hints，建立一个能够准确估计confidence的模型。

## Method


![](/images/blog/confidence_hint_nmt.png)

加入了一个小网络ConNet，输入 $h_t$ 预测一个0到1的数 $c_t$ ，表示模型需要看到多少提示。
模型输出的概率分布就是真实的概率分布和标签（hint）的加权。

$$p_t' = c_t \cdot p_t + (1 - c_t) \cdot y_t$$

模型看到提示需要有惩罚，加上惩罚项：

$$\mathcal{L}_{Conf}=\sum_{t=1}^T-log(c_t)$$

因为获得hint会受到惩罚，所以当模型对结果很确定时，就不需要获得hint。但当模型不确定时，模型会倾向于获得一些hint来使得输出的概率分布更加准确，降低NMT模型的loss，以增大conf loss为代价。

# ICML21 Order-Agnostic Cross Entropy for Non-Autoregressive Machine Translation

## Motivation

NAT建模词语之间的顺序是很困难的，以往的工作会去寻找一个预测结果和target单调对齐的方式来放宽CE Loss的惩罚。但是单调对齐的方式还是会惩罚word reordering的情况，而这种情况在NAT中仍然很常见。本文为非自回归翻译提出一个新的损失函数：OAXE Loss。将翻译看成集合预测问题，也就是去掉了对词序错误的惩罚，使得loss更加符合NAT输出的句子的质量。

## Method

![](/images/blog/OAXE.png)

a是传统的交叉熵损失，会对每个错误的次序都进行惩罚。
b是单调对齐的XE loss，不会惩罚没有对上的token。但是当预测的结果换了顺序，比如this afternoon的顺序到了句子开头，这部分没有正确地计算loss。
c是本文提出的OAXE loss，公式为：

$$\mathcal{L}_{OAXE}=argmin_{O^i \in O}(-logP(O^i|X))$$

其中 $O$ 是整个排序空间。对于一个长度为N的句子，有N！种排序。之前一篇论文也涉及到这个loss，也有同学问这个搜索空间太大了是如何解决的，但是当时没看这篇论文。原来它也是当作二分图匹配的问题用匈牙利算法将O（N！）的时间复杂度降低到O（$N^3$）。

在训练的时候，直接拿这个loss训练就没有词序信息了，所以可以：
1. 先使用CE loss训练，作为初始化。学到了一个比较好的词序后再用OAXE loss训练。这种方法效果最好。
2. 两种loss进行加权，对CE loss进行衰减。

上面的做法对于保留词序还是不够，作者采用loss truncation的方法，只学习预测概率高于截断参数的词。



# COLING22 ngram-OAXE : Phrase-Based Order-Agnostic Cross Entropy for Non-Autoregressive Machine Translation

## Motivation

对 OAXE loss 进行改进。因为顺序颠倒但意思相同的情况经常出现在短语之间，出现在词之间经常会导致错误。所以本文的loss是将ngram的短语作为集合预测，但是要求短语内部仍然保持顺序。

## Method

![](/images/blog/ngram_oaxe.png)

1. 构建target句子中ngrams的概率分布  
2. 找到ngrams的最佳顺序，也就是能让交叉熵最小的顺序  

$Y=\{y_1,\dots,y_I\}$，其中的ngrams是 $Y:\{y_{1:N},\dots,y_{I-N+1:I}\}$。
ngram的概率分布是：

$$P_G(y_{i:i+N-1}|X)=\prod_{i=1}^{i+N-1}P(y_t|X)$$

本文的loss变成了：

$$\mathcal{L}_{ngram-OAXE}=min_{O^j\in O}(-logP_G(O^j|X))$$

这里的 $O^j$ 需要满足：
1. 不能有重叠的短语。
2. 短语长度可以不一样。

对于一个长度为 I 的句子，因为每个词都可以作为一个短语的开头或非开头，所以有 $2^I$ 种切分方式。所有切分方式的期望短语个数是 I/2，如果用匈牙利算法，时间复杂度就是 $O((I/2)^3)$。所以最终的时间复杂度是 $O(2^II^3)$。为了快速搜索，放宽条件：
1. 考虑句子中的所有ngram，也就是可以有重叠的短语，一个词可以出现在多个短语中。
2. 短语长度固定，论文里设置为2。可以用匈牙利算法。

最终时间复杂度为 $O(I^3)$ ，对顺序 $O^j$ 的loss计算公式为：

$$P_G(O^j|X)=\prod_{y_{i:i+N-1}\in O^j}P_G(y_{i:i+N-1}|X)$$

