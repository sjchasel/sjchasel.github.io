---
layout: post
title: 【论文阅读】两篇NAT工作
categories: 论文笔记
keywords: NAT, NMT, GEC, NLP
mathjax: true
---

# Incorporating a Local Translation Mechanism into Non-autoregressive Translation

EMNLP 2020
code: https://github.com/shawnkx/NAT-with-Local-AT


## Motivation

NAT，大概就是，加速。
因为NAT不会考虑太多输出的token之间的依赖关系，所以重复和不完整问题很严重。以往的方法是迭代式修正，而作者在这篇论文提出了一个新的NAT机制——local autoregressive translation (LAT)。
（之前听论文也有讲过这个，2020年的这篇是第一篇？）
可以捕捉token的一些局部的依赖关系，并且同样很快。

## Method——CMLM with LAT

### CMLM

或许我应该浅浅了解一下CMLM？

Mask-Predict: Parallel Decoding of Conditional Masked Language Models
来源：EMNLP2019
链接：https://arxiv.org/pdf/1904.09324.pdf
代码：https://github.com/facebookresearch/Mask-Predict

首先非自回归式地生成所有的token，然后对置信度较低的token重新预测、迭代进行。
将BERT作为解码器，因为BERT可以还原输入的句子中mask掉的位置。给BERT拼上一个编码器，首先根据CLS预测目标句子的token个数，比如是N。然后把mask符号复制N次给BERT进行解码，即预测整个target序列。我们把这个序列里置信度较低的几个token继续mask掉，给BERT还原。重复。每一轮要mask的token数量应该是越来越少的。所以，在训练时，mask的个数随着轮数线性衰减。

### Model

对于每个解码的position，不是只解码出一个token，而是自回归式地解码出一个短序列（a translation piece）。
但是这样解码出来的相邻的短序列之间可能会有重复，于是又设计了一个算法，将短序列对齐和合并，成为一个正常的输出。这个算法的假设是，每个短序列自己都是流畅的，相邻的短序列之间重复的token就是它们需要对齐合并的地方。

#### Decoding

模型是嵌入在CMLM上的，因此CMLM的decoder是BERT，在每一步解码出一个hidden vector，作者采用一个LSTM作为它的decoder，根据这个hidden vector继续解码出K个token（这里设为3）。
同样采用了迭代式预测的做法，mask置信度较低的token。因为作者的方法后续还要经过合并算法的调整，所以预测的target长度准不准都无所谓，只要不太离谱就行。

#### Training

N：句子长度

和CMLM的训练过程相似，sample一个从1到N的数字，mask这么多个token。对于每个token的position，我们都需要预测一个短序列，这里是3个token。如果被mask的是第i个token，那么需要预测的短序列就是 $t_{i}, t_{i+1}, t_{i+2}$。
在算loss的时候，序列里的所有token都要计算。但是会更关注被mask的token，没被mask过的token会加个权。

$$
\begin{aligned}
\mathcal{L}=&-\sum_{i=1}^{N} \sum_{j=1}^{K} \mathbb{1}\left\{t_{i}^{j} \in T_{\text {mask }}\right\} \log \left(p\left(t_{i}^{j}\right)\right) \\
&-\sum_{i=1}^{N} \sum_{j=1}^{K} \mathbb{1}\left\{t_{i}^{j} \notin T_{\text {mask }}\right\} \alpha \log \left(p\left(t_{i}^{j}\right)\right)
\end{aligned}
$$


还会随机删除一些token？让模型学习insertion的操作。
这里的 $\alpha$ 作者是设成了0.1，最后的loss还会再加一个对target长度的预测。

### Merging Algorithm


在inference的时候，每个position都会继续解码出3个token。如果自回归过程训练得很好的话，那么每个子序列内部是有序且被正确翻译的，那么它们之间就会有重合的token，我们就根据这些重合的token来对齐和合并。

![example](/images/blog/nat_merge.png)

如上图所示，当有s1和s2两个子序列需要合并时，先找出它们的Longest Common Subsequence。如果没有的话就直接把这两个子序列拼起来。如果有的话，我们需要解决它们起冲突的token。同样是根据预测的置信度选取要保留的tokens。关于最终序列的长度，还会随机插入或删除一些mask符号来进行调整。


## Experiment

![exp](/images/blog/lat_main.png)

相比于之前的CMLM，LAT可以在更小的迭代次数时达到相当或更好的效果。

其他的一些参数分析实验：
1. 短序列的长度增加，效果可能会有略微微的提高（过长的时候也下降了），速度变慢。
2. CMLM在迭代次数不够时，重复问题很严重。但是因为LAT有合并算法，所以没什么重复问题。
3. LAT在长句子上会比CMLM更好。

# FastCorrect: Fast Error Correction with Edit Alignment for Automatic Speech Recognition

NIPS 2021
code: https://github.com/microsoft/NeuralSpeech/tree/master/FastCorrect

语音识别里的改错？

## Motivation

ASR识别出来的句子需要进行改错，以往都是用seq2seq的模型来纠错，但这样速度很慢，不利于部署在线服务。受到NAT的启发，可以构建非自回归式的模型来对ASR的输出进行纠错。

如果直接使用NMT里比较好的模型，甚至还会在这个任务上表现更差。可能是因为在翻译里，src的每个token都是需要改变的，但是在ASR中不是。并且ASR中需要改正的token识别出来也很困难。所以想把非自回归模型应用在ASR中，需要根据这个任务的特点精心设计。

## Model

### Overview

在纠错模型中通常有三种操作：insertion, deletion, and substitution。受此启发，作者构建的FastCorrect xxx。
模型首先需要预测target的长度，然后非自回归地生成target。


在ASR中，src和tgt是单调对齐的。ASR的acc是根据编辑距离来衡量的WER（word error rate）。

**Training：** FastCorrect首先获得一个operation path，其中包含三种动作。通过这个path，src可以变成tgt。在进行这些操作后，提取tgt中有多少token是和src对齐的。这个长度就是需要预测的length。
**Inference：** 首先预测tgt的长度，根据这个长度来调整src。

### Model——FastCorrect


#### Edit Alignment

这边的操作是需要得到一串数字，src有几个，这串数字就有几个。是的，每个src token对应一个数字，这个数字表示在tgt中和src这个token的相关token有多少个。所以是一种对齐的操作。

在后续的模型中，我们是先预测这串数字，再通过这串数字修正src，输入到解码器中让decoder决定要做什么操作：保留、替换。

![edit](/images/blog/fastcorrect_edit.png)

**1. 计算具有最小编辑距离的编辑路径（左图和中图）**

获得两个句子之间最小编辑距离的方法是：递归地计算两个前缀序列的编辑距离，也就是

$$
D(i, j)=\min \left(D(i-1, j)+1, D(i, j-1)+1, D(i-1, j-1)+\mathbb{1}\left(s_{i} \neq t_{j}\right)\right)
$$

其中，D(i,j)表示src的前i个token和tgt的前j个token的编辑距离，最后空心的1是：当括号内的条件成立的时候就为1，否则为0。

只要算出第一列和第一行，我们就可以递归地计算每两个前缀序列之间的编辑距离。详情请看！这个视频：[Minimum Edit Distance Dynamic Programming](https://www.youtube.com/watch?v=We3YDTzNXEk)——
好的，全懂了。
edit path，就是最短编辑距离是如何计算过来的。这边有三条路。我们选择src和tgt重合最多的，即a和b。

**2. choosing edit alignment with the highest n-gram frequency（右图）**

当选出edit path后，需要对其src和tgt，这一步的变数仅在于插入操作，可以选择是前面那个token的插入，或者后面那个token的插入。
有了不同的对齐方式后如何选择最合适的一条呢？  
这里我们不考虑小于2的数字，即只考虑相关token为2的位置（发生了插入操作的位置）。如果这两个token在src的出现频率高，就很好！因为这样模型预测起来更简单更自然。

#### Model Architecture

![model](/images/blog/fastcorrect_model.png)

模型基于transformer，src输入encoder后，输出的每个hidden state都去预测当前position会在tgt中与之相关的token的个数。如果是2，说明是插入操作，如果是1，说明是替换或保留操作，如果是0，说明是删除操作。

输进解码器的序列就是原序列的token乘预测出来的长度。比如这里第一个B输出的是2，说明它后面需要插入一个字符，那么就将B复制两次输入，让decoder去修改其中一个为插入的字符。第二个B和第三个D和最后一个F都是1，它们可能是需要替换，也可能是保留，让decoder去决定。E需要被删除，那么就删掉，不再输入decoder。

因为ASR的数据里大部分都是正确的token，只有小部分需要修改，不好训模型。所以作者爬了很多数据进行插入、删除、替换操作，构造伪数据去对模型进行预训练，然后再在任务数据集上fine-tune。

## Experiment

![exp](/images/blog/fastcorrect_main.png)


精度高，速度快！




