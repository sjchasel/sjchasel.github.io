---
layout: post
title: 【论文阅读】week4 paper reading
categories: 论文笔记
keywords: NLP, KPG, KPE
mathjax: true
---

两篇ppr论文

![](/images/blog/2022-10-05.png)


两篇还要加在综述里的论文

# IPM18 Local word vectors guiding keyphrase extraction

## Motivation  

单词和短语的local representation在捕捉它们所在的document中的语义有更好的效果，因此很适合用作kpe。因此本文不适用预训练的向量，而是在单文档上学习loca GloVe representation。

![](/images/blog/local_method.png)

## Method

### candidate keyphrases' production


+ unigrams的字母是2-36个，不包含停用词、数字、符号
+ bigrams、trigrams的单词长度都要大于4个字母

### Scoring the candidate keyphrases

+ 在单文档上计算GloVe vectors。因为是在单文档上，所以叫local representation。
+ 排序unigrams：计算reference vector，即所有candidate unigrams的vector的平均。求每个unigram和reference vector的余弦相似度。
+ 排序bigrams和trigrams：这些短语包含的所有单词的分数之和。这样做分数会和短语的长度相关，但这是因为我们更喜欢长的短语，以前也有工作是直接给单词分数加和而不是取平均。

# Experiment

![](/images/blog/local_setting.png)

![](/images/blog/local_res.png)

# TASLP21 Addressing Extraction and Generation Separately: Keyphrase Prediction With Pre-Trained Language Models  

#  Motivation

因为想要同时获得absent kp，因此现在两种kp都用序列到序列的模型生成，但实际情况是present kp大多都通过模型中类似于抽取操作的copy机制得到的。但是copy机制只会在当前步决定要不要copy词，是没有一个全局的视野，无法建模决策之间的相互依赖关系。present kp由这种方式生成，其实是对效果有损害的。因此有必要协调一下两种kp的生成方式。   

# Method


论文将其拆分成PKE和AKG。PKE首先由一个 bert-based sentence selector 去选择可能包含present kp的句子，再对这些句子用BiLSTM-CRF进行序列标注获得present kp。而AKG采用带有门控机制的transformer去融合之前标注出来的present kp的知识，来进行absent kp的生成（One2One），最后用一个BERT-based的模型来对absent kp排序（根据kp和文档的语义相似度）。

![](/images/blog/taslp_kpg1.png)

如上图所示，三个模块分别为：BERT-PKE, BERT-AKG, and Reranker。
训练的时候要先将present kp的extractor训好，然后再训生成absent的模块（以一个在PKE任务上fine-tune好的BERT作为初始化）。

![](/images/blog/taslp_kpg2.png)

# Experiment

![](/images/blog/taslp_kpg_exp.png)