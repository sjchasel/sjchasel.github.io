---
layout: post
title: 【论文阅读】week4 paper reading
categories: 论文笔记
keywords: NLP, KPG, KPE
mathjax: true
---

两篇ppr论文：

![](/images/blog/2022-10-05.png)


还要加在综述里的论文：

# IPM19 A multi-centrality index for graph-based keyword extraction

基于图的无监督抽取方法，抽取的是keyword。不是对图的构造做文章，只是在一个单纯的无向无权共现词图上比较了九种计算中心度的方法，提出了一个组合方式来识别重要的词。

# IPM18 Local word vectors guiding keyphrase extraction

## Motivation  

单词和短语的local representation在捕捉它们所在的document中的语义有更好的效果，因此很适合用作kpe。因此本文不适用预训练的向量，而是在单文档上学习loca GloVe representation。

![](/images/blog/local_method.png)

## Method

### candidate keyphrases' production


+ unigrams的字母是2-36个，不包含停用词、数字、符号
+ bigrams、trigrams的单词长度都要大于4个字母

### Scoring the candidate keyphrases

+ 在单文档上计算GloVe vectors。因为是在单文档上，所以叫local representation。
+ 排序unigrams：计算reference vector，即所有candidate unigrams的vector的平均。求每个unigram和reference vector的余弦相似度。
+ 排序bigrams和trigrams：这些短语包含的所有单词的分数之和。这样做分数会和短语的长度相关，但这是因为我们更喜欢长的短语，以前也有工作是直接给单词分数加和而不是取平均。

# Experiment

![](/images/blog/local_setting.png)

![](/images/blog/local_res.png)



# TASLP21 Addressing Extraction and Generation Separately: Keyphrase Prediction With Pre-Trained Language Models  

##  Motivation

+ absent kp和present kp的差异大，现在统一建模的方式会损害present kp的产生效果，因此将present kp和absent kp分别建模。在这一类论文中这大概是分的最开的，成了两个没有梯度传递的单独的模型。
+ 用present的信息引导absent的生成，因为present kp的信息有文章的主题信息。这个信息是通过预训练的embedding传递的。

## Method


论文将其拆分成PKE和AKG。PKE首先由一个 bert-based sentence selector 去选择可能包含present kp的句子，再对这些句子用BiLSTM-CRF进行序列标注获得present kp。  
而AKG采用带有门控机制的transformer去融合之前标注出来的present kp的知识，来进行absent kp的生成（One2One）。这里的知识指的是，BERT-PKE在经过PKEfine-tune后，这个BERT中就包含了present keyphrase的信息。AKG是基于transformer的，transformer的encoder对文档编码，也会用PKE fine-tune后的BERT对文档编码的embedding，将二者做一个融合，就用到了之前present keyphrase的知识。
最后用一个BERT-based的模型来对absent kp排序（根据kp和文档的语义相似度）。

![](/images/blog/taslp_kpg1.png)

如上图所示，三个模块分别为：BERT-PKE, BERT-AKG, and Reranker。
训练的时候要先将present kp的extractor训好，然后再训生成absent的模块（以一个在PKE任务上fine-tune好的BERT作为初始化）。

![](/images/blog/taslp_kpg2.png)

## Experiment

![](/images/blog/taslp_kpg_exp.png)



和工作相关的论文：

# NAACL22 Diversifying Neural Dialogue Generation via Negative Distillation

具体做法大概可以详细写写。


