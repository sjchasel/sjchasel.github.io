---
layout: post
title: 【论文阅读】weekly paper reading - 1
categories: 论文笔记
keywords: NLP, NMT, KPE
mathjax: true
---



# MDERank

MDERank: A Masked Document Embedding Rank Approach for Unsupervised Keyphrase Extraction  


## Motivation
把EmbedRank和SIFRank这种计算文档和短语相似度的方法称为Phrase-Document-based (PD) methods。这类方法的缺陷在于：
1. 文档比短语长很多，很难准确计算它们在同一个latent space中的相似性。这类方法会倾向于提取长的短语。
（统计/图抽取方法会倾向于抽取短的短语，这两种方法能结合一下吗？）
2. 短语的embedding计算缺少上下文语境，导致计算出来的相似度也是不准确的。
本文提出的方法是Document-Document method。也就是代表短语的embedding也是document级别的。

## Method

![](/images/blog/mderank.png)

将候选短语一个一个地mask掉，通过作者训练的KPEBERT来计算mask后文档的embedding，和原始的文档embedding计算余弦相似度。相似度越高，说明mask这个短语对文档的embedding的改变很小，说明这个短语可有可无，不是关键短语。相似度越低，作为关键短语的得分就越高。

而对于KPEBERT，作者是加入了对比学习的loss。
![](/images/blog/KPEBERT.png)
但是作者没有开源这个模型。用了KPEBERT的MDERank没有在所有数据集上都比SIFRank好，也没有在kp20k上的测试。还是试试开源的SIFRank。

# SIFRank


![](/images/blog/sifrank.png)

SIFRank采用的是预训练的sentence embedding模型——SIF来得到短语的embedding和文档的embedding。根据SIFRank的特点，它会更注重内容的主题，所以很适合用在这个任务上。
（EmbedRank也有用sent2vec模型来获得短语和文档的embedding的，感觉预训练的sentence embedding来同时获得短语和文档的embedding可以说是对这两种长度差异很大的文本的折中表示。）
word embedding通过ELMO获得。





# 短语级复述的识别与抽取

刘挺老师的硕士在09年的毕业论文，如何自动从互联网数据抓取和识别复述短语。  
这篇论文里提出的一些获得复述短语的方法：
1. 利用词典进行同义词替换
2. 反译
3. 短语的上下文词袋

这篇论文中使用的也是第三种，稍微改进了一些。  
复述短语可以分成相同词部分和复述部分，可以基于相同词提取复述短语，也可以基于句法结构来提取。我们的第一步筛选就是在找包含相同词部分的短语。  
对于基于句法结构的方法，作者观察到“如果两个相同成分的短语依赖于同一个词，那么它们为复述的可能性很大”。作者有可能为复述句的两个句子，可以分析其依存结构，但我们没有这样的句子。可以使用T5去生成复述句，然后提取和原句中present kp依存结构相同的短语？
对于候选短语的过滤，作者也提到长度相差过大的短语要去掉。

![](/images/blog/phrase_filter.png)

从候选短语中确定正确的复述短语，作者将其视为一个二分类问题。构造了8个统计特征，利用SVM分类。这些特征可以帮助我们进一步筛选候选短语，也可以结合词袋模型或者预训练模型得到的embedding的相似性得分来计算最后的得分。

<!-- 1. 短语串相似特征
    1. 词长比特征。len(a)/len(b) 当len(a)<len(b)
    2. 词编辑距离特征。本文对编辑距离用短语的长度做了进一步优化：$F_{wed}(s,t)=\frac{dist(s,t)}{(m+n)/2}$
    3. 词重叠特征。$F_{sw}(s,t)=\frac{overlap(s,t)}{(m+n)/2}$ -->

# 翻译论文两篇

![](/images/blog/week2_nmt2.png)

<!-- ## 【NIPS22】Refining Low-Resource Unsupervised Translation by Language Disentanglement of Multilingual Model -->

<!-- ### Motivation

在对低资源语种做无监督翻译时，总是需要一些高资源的语言counterpart。虽然这些高资源语言可以帮助低资源语言翻译的启动，但是语言之间的差异会影响低资源语言翻译性能的进一步提高。并且语言越多，模型效果就越差。
作者从预训练的无监督多语言模型理顺一些无关的语言。

### Method

#### 新的FFN结构


#### 四阶段

### Method

## 【ICML21】Cross-model Back-translated Distillation for Unsupervised Machine Translation

### Motivation

在UNMT中，back translation提供了diverse data，但这种方法似乎到了性能提升的瓶颈阶段。本文提出一种新的（another level）提供diverse data的方法，是以往方法（iterative back translation 和 denosing auto- encoding for language modeling 所缺的）

### Method

UMT的三个principal分别为：model initialization, language
modeling ( denoising auto-encoding, DAE) and iterative back-translation (IBT). 第一点可以用训好的翻译模型对UMT模型初始化，也可以用预训练模型。后面两个都是 data diversification 过程。但随着模型训练时间的变化，这两种方法对效果的提升逐渐平缓。作者觉得这两种方法到达了瓶颈，于是提出了一种新的 data diversification 过程：Cross-model Back-translated Distillation (CBD)。  

需要两个双向的UMT模型。
1. 一个模型从s翻译到t
2. 另一个模型从t翻译到s
3. 前两步的数据组合起来蒸馏出一个有监督模型 -->

