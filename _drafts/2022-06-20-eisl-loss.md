---
layout: post
title: 【论文阅读】Don’t Take It Literally： An Edit-Invariant Sequence Loss for Text Generation
categories: 论文笔记
keywords: NLP
mathjax: true
---



NAACL 2022
code: https://github.com/guangyliu/EISL

# Motivation

这是对文本生成中loss做的改进。
传统的CE loss使得模型exact token-by-token match target序列，这是over-restrictive。有些句子是target的paraphrase，它在语义上和target很接近，却会被很重地惩罚。除此之外，对于一些target有噪声、只能使用弱监督（？）数据的时候，CE loss也不适用，会误导模型。
作者在三个场景上进行了实验：
1. target有噪声的机器翻译
2. 只有弱监督信号的text style transfer 
3. 非自回归的生成（没有预定义的生成顺序）

所以重点是看看这篇提出的loss能不能提升非自回归生成。